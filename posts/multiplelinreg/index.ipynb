{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multiple Liear Regression\n",
        "\n",
        "Rafiq Islam  \n",
        "2024-08-29\n",
        "\n",
        "## Multiple Linear Regression\n",
        "\n",
        "The multiple linear regression takes the form  \n",
        "$$\n",
        "y=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\cdots +\\beta_d x_d+\\xi=\\vec{x}\\cdot \\vec{\\beta}+\\xi\n",
        "$$\n",
        "\n",
        "with $\\{\\beta_i\\}_{i=0}^{d}\\in \\mathbb{R}$ constants or parameters of\n",
        "the model. In vector notation, $\\vec{\\beta}\\in \\mathbb{R}^{d+1}$,\n",
        "\n",
        "$$\n",
        "\\vec{\\beta}=\\begin{pmatrix}\\beta_0\\\\ \\beta_1\\\\ \\vdots \\\\ \\beta_d \\end{pmatrix};\\hspace{4mm}\\vec{x}=\\begin{pmatrix}1\\\\ x_1\\\\ x_2\\\\ \\vdots\\\\ x_d\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "For $n$ data points, in matrix algebra notation, we can write\n",
        "$y=X\\vec{\\beta}+\\xi$ where $X\\in \\mathcal{M}_{n\\times (d+1)}$ and\n",
        "$y\\in \\mathbb{R}^{d+1}$ with\n",
        "\n",
        "$$\n",
        "X=\\begin{pmatrix}1&x_{11}&x_{12}&\\cdots&x_{1d}\\\\1&x_{21}&x_{22}&\\cdots&x_{2d}\\\\ \\vdots& \\vdots &\\vdots&\\ddots &\\vdots\\\\1&x_{n1}&x_{n2}&\\cdots&x_{nd} \\end{pmatrix};\\hspace{4mm} y=\\begin{pmatrix}y_1\\\\y_2\\\\ \\vdots\\\\ y_n\\end{pmatrix};\\hspace{4mm} \\xi=\\begin{pmatrix}\\xi_1\\\\ \\xi_2\\\\ \\vdots\\\\ \\xi_n\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "We fit the $n$ data points with the objective to minimize the loss\n",
        "function, mean squared error\n",
        "\n",
        "$$\n",
        "MSE(\\vec{\\beta})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i-f_{\\vec{\\beta}}(\\vec{x}_i)\\right)^2=\\frac{1}{n}\\left|\\vec{y}-X\\vec{\\beta}\\right|^2\n",
        "$$\n",
        "\n",
        "## Ordinary Least Square Method"
      ],
      "id": "3f7137f8-2633-4141-b212-60f427352511"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "135d03c0-487f-45b5-9157-dbb55ed5e5da"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `scikit-learn` library uses ***Ordinary Least Squares (OLS)***\n",
        "method to find the parameters. This method is good for a simple and\n",
        "relatively smaller dataset. Here is a short note on this method.\n",
        "However, when the dimension is very high and the dataset is bigger,\n",
        "`scikit-learn` uses another method called ***Stochastic Gradient\n",
        "Descent*** for optimization which is discussed in the next section."
      ],
      "id": "b6e20c32-d15b-436e-ace0-a800b710b22d"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "a35a8b1b-106f-43c0-a919-604f6c381379"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The goal of OLS is to find the parameter vector $\\hat{\\beta}$ that\n",
        "minimizes the sum of squared errors (SSE) between the observed target\n",
        "values $y$ and the predicted values $\\hat{y}$:\n",
        "\n",
        "$$\n",
        "\\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - X_i\\beta)^2\n",
        "$$\n",
        "\n",
        "This can be expressed in matrix form as:\n",
        "\n",
        "$$\n",
        "\\text{SSE} = (y - X\\beta)^T(y - X\\beta)\n",
        "$$\n",
        "\n",
        "To minimize the SSE, let’s first expand the expression:\n",
        "\n",
        "Since $\\beta^T X^T y$ is a scalar (a 1x1 matrix), it is equal to its\n",
        "transpose. That is\n",
        "\n",
        "and therefore,\n",
        "\n",
        "$$\n",
        "\\text{SSE} = y^T y - 2\\beta^T X^T y + \\beta^T X^T X \\beta\n",
        "$$\n",
        "\n",
        "To find the minimum of the SSE, we take the derivative with respect to\n",
        "$\\beta$ and set it to zero:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\text{SSE}}{\\partial \\beta} = -2X^T y + 2X^T X \\beta = 0\n",
        "$$\n",
        "\n",
        "Now, solve for $\\beta$:\n",
        "\n",
        "$$\n",
        "X^T X \\beta = X^T y\n",
        "$$\n",
        "\n",
        "To isolate $\\beta$, we multiply both sides by $(X^T X)^{-1}$ (assuming\n",
        "$X^T X$ is invertible):\n",
        "\n",
        "$$\n",
        "\\beta = (X^T X)^{-1} X^T y\n",
        "$$"
      ],
      "id": "f179fd6a-3d16-4c0f-b025-582142623b23"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "5100b012-1e71-43f4-97b3-73f65a1a4778"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The vector $\\hat{\\beta} = (X^T X)^{-1} X^T y$ gives the estimated\n",
        "coefficients that minimize the sum of squared errors between the\n",
        "observed target values $y$ and the predicted values\n",
        "$\\hat{y} = X\\hat{\\beta}$. This method is exact and works well when\n",
        "$X^T X$ is invertible and the dataset size is manageable. <br> <br> This\n",
        "method is very efficient for small to medium-sized datasets but can\n",
        "become computationally expensive for very large datasets due to the\n",
        "inversion of the matrix $X^TX$."
      ],
      "id": "c43e5897-e27a-4111-808a-6ae173130862"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "47bb6236-87ad-4dd2-a3e1-de3874533baa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Iterative Method\n",
        "\n",
        "### Gradient Descent"
      ],
      "id": "62a614fc-9e2c-49d8-88d3-7a2dde99fc05"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "c409ae0e-9002-424e-80ca-8e655e1c95ea"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img align=\"right\" height=350 width=450 src=\"/dsandml/multiplelinreg/gradient_descent.gif\" alt=\"Collected from gbhat.com\" style=\"margin-left: 20px; margin-bottom: 20px\"/>\n",
        "<br> GIF Credit:\n",
        "<a href=\"https://gbhat.com/machine_learning/gradient_descent_anim.html\" target=\"_blank\" style=\"text-decoration:none\">gbhat.com</a>  \n",
        "<br> Gradient Descent is an optimization algorithm used to minimize the\n",
        "cost function. The cost function $f(\\beta)$ measures how well a model\n",
        "with parameters $\\beta$ fits the data. The goal is to find the values of\n",
        "$\\beta$ that minimize this cost function. In terms of the iterative\n",
        "method, we want to find $\\beta_{k+1}$ and $\\beta_k$ such that\n",
        "$f(\\beta_{k+1})<f(\\beta_k)$. <br> <br> For a small change in $\\beta$, we\n",
        "can approximate $f(\\beta)$ using Taylor series expansion  \n",
        "$$f(\\beta_{k+1})=f(\\beta_k +\\Delta\\beta_k)\\approx f(\\beta_k)+\\nabla f(\\beta_k)^T \\Delta \\beta_k+\\text{higher-order terms}$$"
      ],
      "id": "c41c9705-4fde-45f3-8dad-4bfdef8bac67"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "4e019f2b-8346-450a-8e08-9f6f2780d9ff"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The update rule for vanilla gradient descent is given by:\n",
        "\n",
        "$$\n",
        "\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "-   $\\beta_k$ is the current estimate of the parameters at iteration\n",
        "    $k$.\n",
        "-   $\\eta$ is the learning rate, a small positive scalar that controls\n",
        "    the step size.\n",
        "-   $\\nabla f(\\beta_k)$ is the gradient of the cost function $f$ with\n",
        "    respect to $\\beta$ at the current point $\\beta_k$."
      ],
      "id": "d2eabd1f-4b1f-4cde-b660-8ef09eb6373b"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "c97a535a-bebb-4f43-954e-231273880011"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The update rule comes from the idea of moving the parameter vector\n",
        "$\\beta$ in the direction that decreases the cost function the most.\n",
        "\n",
        "1.  **Gradient**: The gradient $\\nabla f(\\beta_k)$ represents the\n",
        "    direction and magnitude of the steepest ascent of the function $f$\n",
        "    at the point $\\beta_k$. Since we want to minimize the function, we\n",
        "    move in the opposite direction of the gradient.\n",
        "\n",
        "2.  **Step Size**: The term $\\eta \\nabla f(\\beta_k)$ scales the gradient\n",
        "    by the learning rate $\\eta$, determining how far we move in that\n",
        "    direction. If $\\eta$ is too large, the algorithm may overshoot the\n",
        "    minimum; if it’s too small, the convergence will be slow.\n",
        "\n",
        "3.  **Iterative Update**: Starting from an initial guess $\\beta_0$, we\n",
        "    repeatedly apply the update rule until the algorithm converges,\n",
        "    meaning that the changes in $\\beta_k$ become negligible, and\n",
        "    $\\beta_k$ is close to the optimal value $\\beta^*$.\n",
        "\n",
        "### Stochastic Gradient Descent (SGD)"
      ],
      "id": "1599c541-3cb0-4345-a5ea-e8fb665f4c4f"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "83b81d32-7a44-4569-9627-52501db1151f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stochastic Gradient Descent is a variation of the vanilla gradient\n",
        "descent. Instead of computing the gradient using the entire dataset, SGD\n",
        "updates the parameters using only a single data point or a small batch\n",
        "of data points at each iteration. The later one we call it mini batch\n",
        "SGD."
      ],
      "id": "242961b4-e6d9-4267-9ca1-72f1cc5e3a83"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "278329cc-87b1-45be-ad2b-078d3d970a98"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Suppose our cost function is defined as the average over a dataset of\n",
        "size $n$:\n",
        "\n",
        "$$\n",
        "f(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} f_i(\\beta)\n",
        "$$\n",
        "\n",
        "Where $f_i(\\beta)$ represents the contribution of the $i$-th data point\n",
        "to the total cost function. The gradient of the cost function with\n",
        "respect to $\\beta$ is:\n",
        "\n",
        "$$\n",
        "\\nabla f(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_i(\\beta)\n",
        "$$\n",
        "\n",
        "Vanilla gradient descent would update the parameters as:\n",
        "\n",
        "$$\n",
        "\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n",
        "$$\n",
        "\n",
        "Instead of using the entire dataset to compute the gradient, SGD\n",
        "approximates the gradient by using only a single data point (or a small\n",
        "batch). The update rule for SGD is:\n",
        "\n",
        "$$\n",
        "\\beta_{k+1} = \\beta_k - \\eta \\nabla f_{i_k}(\\beta_k)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "-   $i_k$ is the index of a randomly selected data point at iteration\n",
        "    $k$.\n",
        "-   $\\nabla f_{i_k}(\\beta_k)$ is the gradient of the cost function with\n",
        "    respect to the parameter $\\beta_k$, evaluated only at the data point\n",
        "    indexed by $i_k$.\n",
        "\n",
        "## Python Execution\n",
        "\n",
        "### Synthetic Data"
      ],
      "id": "2f3ff9fa-1b5f-4c54-85e3-3f6868cd2c6f"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "X=np.random.randn(1000,2)\n",
        "y=3*X[:,0]+2*X[:,1]+1+np.random.randn(1000)"
      ],
      "id": "3855311c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So for this project, our known relationship is $y=1+3x_1+2x_2+\\xi$.\n",
        "\n",
        "### Fit the data: Using scikit-learn Library"
      ],
      "id": "d6a66601-a7b3-4b51-a3e5-971abddcd708"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlr=LinearRegression()\n",
        "mlr.fit(X,y)\n",
        "coefficients=mlr.coef_.tolist()\n",
        "slope=mlr.intercept_.tolist()"
      ],
      "id": "675ce3da"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the model parameters: slope $\\beta_0=$ 0.9836 and coefficients\n",
        "$\\beta_1=$ 3.029, and $\\beta_2=$ 2.0049\n",
        "\n",
        "### Fit the data: Using Custom Library OLS\n",
        "\n",
        "First we create our custom `NewLinearRegression` using the OLS formula\n",
        "above and save this `python` class as `mlreg.py`\n",
        "\n",
        "``` python\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class NewLinearRegression:\n",
        "    def __init__(self) -> None:\n",
        "        self.beta = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n",
        "        X_transpose_X = np.dot(X.transpose(), X)\n",
        "        X_transpose_X_inverse = np.linalg.inv(X_transpose_X)\n",
        "        X_transpose_y = np.dot(X.transpose(), y)\n",
        "        self.beta = np.dot(X_transpose_X_inverse, X_transpose_y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n",
        "        return np.dot(X, self.beta)\n",
        "\n",
        "    def coeff_(self):\n",
        "        return self.beta[1:].tolist()\n",
        "\n",
        "    def interceptt_(self):\n",
        "        return self.beta[0].tolist()\n",
        "```\n",
        "\n",
        "Now it’s time to use the new class"
      ],
      "id": "debfb4ca-417c-49e3-a956-5470feae6cac"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlreg import NewLinearRegression\n",
        "mlr1 = NewLinearRegression()\n",
        "mlr1.fit(X,y)\n",
        "coefficients1=mlr1.coeff_()\n",
        "slope1=mlr1.interceptt_()"
      ],
      "id": "186a1efb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the model parameters: slope $\\beta_0=$ 0.9836 and coefficients\n",
        "$\\beta_1=$ 3.029, and $\\beta_2=$ 2.0049\n",
        "\n",
        "### Fit the data: Using Gradient Descent\n",
        "\n",
        "We create the class\n",
        "\n",
        "``` python\n",
        "class GDLinearRegression:\n",
        "    def __init__(self, learning_rate=0.01, number_of_iteration=1000) -> None:\n",
        "        self.learning_rate = learning_rate\n",
        "        self.number_of_iteration = number_of_iteration\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        num_of_samples, num_of_features = X.shape\n",
        "        self.weights = np.zeros(num_of_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.number_of_iteration):\n",
        "            y_predicted = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "            d_weights = (1 / num_of_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            d_bias = (1 / num_of_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            self.weights -= self.learning_rate * d_weights\n",
        "            self.bias -= self.learning_rate * d_bias\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_predicted = np.dot(X, self.weights) + self.bias\n",
        "        return y_predicted\n",
        "\n",
        "    def coefff_(self):\n",
        "        return self.weights.tolist()\n",
        "\n",
        "    def intercepttt_(self):\n",
        "        return self.bias\n",
        "```\n",
        "\n",
        "Now we use this similarly as before,"
      ],
      "id": "4bf17634-7940-42e2-a319-ca22897847d8"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlreg import GDLinearRegression\n",
        "mlr2= GDLinearRegression(learning_rate=0.008)\n",
        "mlr2.fit(X,y)\n",
        "coefficients2=mlr2.coefff_()\n",
        "slope2=mlr2.intercepttt_()"
      ],
      "id": "b90fb1bd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the model parameters: slope $\\beta_0=$ 0.9832 and coefficients\n",
        "$\\beta_1=$ 3.028, and $\\beta_2=$ 2.0041\n",
        "\n",
        "### Fit the data: Using Stochastic Gradient Descent\n",
        "\n",
        "First we define the class\n",
        "\n",
        "``` python\n",
        "class SGDLinearRegression:\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000, batch_size=1) -> None:\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.batch_size = batch_size\n",
        "        self.theta = None\n",
        "        self.mse_list = None  # Initialize mse_list as an instance attribute\n",
        "\n",
        "    def _loss_function(self, X, y, beta):\n",
        "        num_samples = len(y)\n",
        "        y_predicted = X.dot(beta)\n",
        "        mse = (1/num_samples) * np.sum(np.square(y_predicted - y))\n",
        "        return mse\n",
        "\n",
        "    def _gradient_function(self, X, y, beta):\n",
        "        num_samples = len(y)\n",
        "        y_predicted = X.dot(beta)\n",
        "        grad = (1/num_samples) * X.T.dot(y_predicted - y)\n",
        "        return grad\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Adding the intercept term (bias) as a column of ones\n",
        "        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n",
        "        num_features = X.shape[1]\n",
        "        self.theta = np.zeros((num_features, 1))\n",
        "\n",
        "        self.mse_list = np.zeros(self.num_iterations)  # Initialize mse_list\n",
        "\n",
        "        for i in range(self.num_iterations):\n",
        "            # Randomly select a batch of data points\n",
        "            indices = np.random.choice(\n",
        "                len(y), size=self.batch_size, replace=False)\n",
        "            X_i = X[indices]\n",
        "            y_i = y[indices].reshape(-1, 1)\n",
        "\n",
        "            # Compute the gradient and update the weights\n",
        "            gradient = self._gradient_function(X_i, y_i, self.theta)\n",
        "            self.theta = self.theta - self.learning_rate * gradient\n",
        "\n",
        "            # Calculate loss for the entire dataset (optional)\n",
        "            self.mse_list[i] = self._loss_function(X, y, self.theta)\n",
        "\n",
        "        return self.theta, self.mse_list\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Adding the intercept term (bias) as a column of ones\n",
        "        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n",
        "        return X.dot(self.theta)\n",
        "\n",
        "    def coef_(self):\n",
        "        # Return the coefficients (excluding the intercept term)\n",
        "        return self.theta[1:].flatten().tolist()\n",
        "\n",
        "    def intercept_(self):\n",
        "        # Return the intercept term\n",
        "        return self.theta[0].item()\n",
        "\n",
        "    def mse_losses(self):\n",
        "        # Return the mse_list\n",
        "        return self.mse_list.tolist()\n",
        "```\n",
        "\n",
        "Now"
      ],
      "id": "5a18e1e6-2f83-414e-953a-00eca7caa5e0"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mlreg import SGDLinearRegression\n",
        "mlr3=SGDLinearRegression(learning_rate=0.01, num_iterations=1000, batch_size=10)\n",
        "theta, _ = mlr3.fit(X, y)"
      ],
      "id": "3dcfd6ff"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the model parameters: slope $\\beta_0=$ array(\\[0.99208047\\]) and\n",
        "coefficients $\\beta_1=$ array(\\[2.99237427\\]), and $\\beta_2=$\n",
        "array(\\[1.97371023\\])\n",
        "\n",
        "Up next [knn regression](../../dsandml/knn/index.qmd)\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "**Share on**\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer.php?u=https://mrislambd.github.io/dsandml/multiplelinreg/\" target=\"_blank\" style=\"color:#1877F2; text-decoration: none;\">\n",
        "\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://mrislambd.github.io/dsandml/multiplelinreg/\" target=\"_blank\" style=\"color:#0077B5; text-decoration: none;\">\n",
        "\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.twitter.com/intent/tweet?url=https://mrislambd.github.io/dsandml/multiplelinreg/\" target=\"_blank\" style=\"color:#1DA1F2; text-decoration: none;\">\n",
        "\n",
        "</a>"
      ],
      "id": "a816b3da-faaf-468f-ac1d-83dcd74b760e"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<script src=\"https://giscus.app/client.js\"\n",
        "        data-repo=\"mrislambd/mrislambd.github.io\" \n",
        "        data-repo-id=\"R_kgDOMV8crA\"\n",
        "        data-category=\"Announcements\"\n",
        "        data-category-id=\"DIC_kwDOMV8crM4CjbQW\"\n",
        "        data-mapping=\"pathname\"\n",
        "        data-strict=\"0\"\n",
        "        data-reactions-enabled=\"1\"\n",
        "        data-emit-metadata=\"0\"\n",
        "        data-input-position=\"bottom\"\n",
        "        data-theme=\"light\"\n",
        "        data-lang=\"en\"\n",
        "        crossorigin=\"anonymous\"\n",
        "        async>\n",
        "</script>"
      ],
      "id": "d3791e42-4785-4d78-8d07-fa2480668fc8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [],
      "id": "37e4d185-c1d4-485c-9e9b-a79d57b74441"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<script async defer crossorigin=\"anonymous\"\n",
        " src=\"https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v20.0\"></script>"
      ],
      "id": "1e11911b-7134-4a57-b419-e1526b960d36"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**You may also like**"
      ],
      "id": "ab849583-e1af-4a3a-a6cb-89d4cfb7430b"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/opt/hostedtoolcache/Python/3.10.16/x64/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  }
}