{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ensemble Methods: Gradient Boosting - A detailed overview\n",
        "\n",
        "Rafiq Islam  \n",
        "2024-10-07\n",
        "\n",
        "## Introduction"
      ],
      "id": "e4ad8ec3-1aa7-4257-9a29-f5d9cf93380a"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "37fab7ed-273f-4049-9ed1-fadc2846b7eb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gradient Boosting is one of the most powerful techniques for building\n",
        "predictive models. It has gained popularity in the realms of both\n",
        "classification and regression due to its flexibility and effectiveness,\n",
        "particularly with decision trees as weak learners."
      ],
      "id": "c4474a48-b8ed-44b0-b1d3-fe20a395a64f"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "eeb100fd-fc8f-49ee-a21d-661c75b6031f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Gradient Boosting?"
      ],
      "id": "d63b56aa-f5c7-46b6-b1c7-ae1d8e36e19e"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "725db0da-290f-4976-b779-1eaa66877bba"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gradient Boosting is an ensemble learning technique where several weak\n",
        "learners (typically decision trees) are combined to form a strong\n",
        "learner. The key idea behind boosting is to train models sequentially,\n",
        "where each new model tries to correct the errors of the previous ones.\n",
        "Gradient Boosting achieves this by minimizing a loss function using\n",
        "gradient descent."
      ],
      "id": "79d07033-6dcc-43ae-8b15-800b23b6b0d6"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "4101a8f4-b8bd-4747-8d12-a40b8399e58d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key Concepts:**\n",
        "\n",
        "-   **Weak Learners**: These are models that are only slightly better\n",
        "    than random guessing. Decision trees with few splits (depth-1 trees)\n",
        "    are commonly used as weak learners.  \n",
        "-   **Sequential Learning**: Models are trained one after another. Each\n",
        "    model focuses on the errors (residuals) made by the previous\n",
        "    models.  \n",
        "-   **Gradient Descent**: Gradient Boosting relies on gradient descent\n",
        "    to minimize the loss function.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "## Mathematical Derivation of Gradient Boosting\n",
        "\n",
        "Let’s consider a regression problem where we aim to predict the target\n",
        "values $y \\in \\mathbb{R}$ using the features $X \\in \\mathbb{R}^d$. We\n",
        "aim to find a function $F(x)$ that minimizes the expected value of a\n",
        "loss function $L(y, F(x))$, where $L$ could be mean squared error or any\n",
        "other appropriate loss function.\n",
        "\n",
        "The idea behind Gradient Boosting is to improve the current model by\n",
        "adding a new model that reduces the loss:\n",
        "\n",
        "$$\n",
        "F_{m+1}(x) = F_m(x) + \\eta h_m(x)\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "-   $F_m(x)$ is the current model after $m$ iterations,  \n",
        "-   $h_m(x)$ is the new weak learner added at iteration $m$,  \n",
        "-   $\\eta$ is the learning rate, which controls how much the new learner\n",
        "    impacts the final model."
      ],
      "id": "edf60392-9ed4-4fe4-95b9-b3f9dedc03b4"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "4990efc6-8426-4e59-bc67-d04e33cbefd7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We aim to minimize the loss function $L(y, F(x))$. At each iteration,\n",
        "Gradient Boosting fits a new model $h_m(x)$ to the negative gradient of\n",
        "the loss function. The negative gradient represents the direction of\n",
        "steepest descent, essentially capturing the errors or residuals of the\n",
        "model. <br> Given a loss function $L(y, F(x))$, we compute the residuals\n",
        "(or pseudo-residuals) as:"
      ],
      "id": "7ce8fe35-5c8d-44ba-8ebd-e385af7a8f9a"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "3994fbdc-45b4-422e-8874-45d0e0baff42"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "r_{i,m} = - \\frac{\\partial L(y_i, F_m(x_i))}{\\partial F_m(x_i)}\n",
        "$$\n",
        "\n",
        "These residuals are then used to fit the new weak learner $h_m(x)$. In\n",
        "the case of squared error (for regression), the residuals simplify to\n",
        "the difference between the observed and predicted values:\n",
        "\n",
        "$$\n",
        "r_{i,m} = y_i - F_m(x_i)\n",
        "$$\n",
        "\n",
        "Thus, the new learner is fit to minimize these residuals.\n",
        "\n",
        "**Steps**\n",
        "\n",
        "**Initialize** the model with a constant prediction: $$\n",
        "F_0(x) = \\arg \\min_{c} \\sum_{i=1}^{n} L(y_i, c)\n",
        "$$ For squared error loss, $F_0(x)$ would be the mean of the target\n",
        "values $y$.\n",
        "\n",
        "**For each iteration $m = 1, 2, \\dots, M$:**\n",
        "\n",
        "-   Compute the residuals: $$\n",
        "    r_{i,m} = - \\frac{\\partial L(y_i, F_m(x_i))}{\\partial F_m(x_i)}\n",
        "    $$  \n",
        "-   Fit a weak learner $h_m(x)$ to the residuals $r_{i,m}$.  \n",
        "-   Update the model: $$\n",
        "    F_{m+1}(x) = F_m(x) + \\eta h_m(x)\n",
        "    $$  \n",
        "-   Continue until a stopping criterion is met (e.g., a fixed number of\n",
        "    iterations or convergence).\n",
        "\n",
        "## Assumptions of Gradient Boosting\n",
        "\n",
        "Gradient Boosting, like any algorithm, comes with its own set of\n",
        "assumptions and limitations. Key assumptions include:\n",
        "\n",
        "1.  **Independence of Features**: Gradient Boosting assumes that the\n",
        "    features are independent. Correlated features can lead to\n",
        "    overfitting.\n",
        "2.  **Weak Learners**: It assumes that weak learners, typically shallow\n",
        "    decision trees, are adequate for capturing the patterns in the data,\n",
        "    though overly complex learners may lead to overfitting.\n",
        "3.  **Additive Model**: The model is additive, meaning it combines weak\n",
        "    learners to improve performance. This makes it sensitive to noisy\n",
        "    data, as adding too many learners might lead to overfitting.\n",
        "\n",
        "## When to Use Gradient Boosting?\n",
        "\n",
        "Gradient Boosting is ideal in the following scenarios:\n",
        "\n",
        "-   **High Predictive Power**: When accuracy is a top priority, Gradient\n",
        "    Boosting often outperforms simpler algorithms like linear regression\n",
        "    or basic decision trees.  \n",
        "-   **Complex Datasets**: It works well with datasets that have complex\n",
        "    patterns, non-linear relationships, or multiple feature\n",
        "    interactions.  \n",
        "-   **Feature Engineering**: It is less reliant on extensive feature\n",
        "    engineering because decision trees are capable of handling mixed\n",
        "    types of features (numerical and categorical) and automatically\n",
        "    learning interactions.  \n",
        "-   **Imbalanced Data**: Gradient Boosting can handle class imbalances\n",
        "    by tuning the loss function, making it suitable for classification\n",
        "    tasks like fraud detection.\n",
        "\n",
        "However, due to its complexity, Gradient Boosting can be computationally\n",
        "expensive, so it’s less ideal for very large datasets or real-time\n",
        "predictions.\n",
        "\n",
        "## Python Implementation of Gradient Boosting\n",
        "\n",
        "Below is a Python implementation using the `scikit-learn` library for a\n",
        "regression problem. We will use the Boston Housing dataset as an\n",
        "example."
      ],
      "id": "1c067066-ef99-4354-860f-9fa8d9fed9f1"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 6.208861361528038"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "target = raw_df.values[1::2, 2]\n",
        "\n",
        "X = data\n",
        "y = target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and fit the Gradient Boosting Regressor\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = gb_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")"
      ],
      "id": "acf5f9f9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example:\n",
        "\n",
        "-   We use the `GradientBoostingRegressor` from `scikit-learn` for a\n",
        "    regression task.  \n",
        "-   We fit the model on the Boston Housing dataset, and predict values\n",
        "    on the test set.  \n",
        "-   The mean squared error (MSE) is used to evaluate the model’s\n",
        "    performance.\n",
        "\n",
        "**Hyperparameters**:\n",
        "\n",
        "-   `n_estimators`: Number of boosting stages to run.  \n",
        "-   `learning_rate`: Controls the contribution of each tree to the final\n",
        "    model.  \n",
        "-   `max_depth`: Limits the depth of the individual decision trees (weak\n",
        "    learners)."
      ],
      "id": "10a5bf2b-5c96-46c5-9a66-b981f95ed93d"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "d14e60ac-7608-4316-9288-faace48ca7e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gradient Boosting is a powerful ensemble technique, particularly\n",
        "effective for both classification and regression tasks. It builds models\n",
        "sequentially, focusing on correcting the mistakes of prior models. While\n",
        "it is computationally expensive and prone to overfitting if not properly\n",
        "regularized, it often achieves state-of-the-art results in predictive\n",
        "tasks."
      ],
      "id": "b748b9f4-dba4-4340-a0ce-f759fc0455a8"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "486a6b30-ea0e-4358-af9d-bb472356ad9f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "## References\n",
        "\n",
        "1.  <a href=\"https://web.stanford.edu/~hastie/ElemStatLearn/\"\n",
        "    target=\"_blank\" style=\"text-decoration:none\"><strong>“The Elements of\n",
        "    Statistical Learning”</strong> by Trevor Hastie, Robert Tibshirani, and\n",
        "    Jerome Friedman</a> (freely available online).\n",
        "2.  **“Pattern Recognition and Machine Learning”** by Christopher M.\n",
        "    Bishop:\n",
        "3.  <a href=\"https://projecteuclid.org/euclid.aos/1013203451\"\n",
        "    target=\"_blank\" style=\"text-decoration:none\"><strong>“Greedy Function\n",
        "    Approximation: A Gradient Boosting Machine”</strong> by Jerome\n",
        "    Friedman</a>  \n",
        "4.  <a\n",
        "    href=\"https://www.jmlr.org/papers/volume2/friedman01a/friedman01a.pdf\"\n",
        "    target=\"_blank\" style=\"text-decoration:none\"><strong>“A Short\n",
        "    Introduction to Boosting”</strong> by Yoav Freund and Robert E.\n",
        "    Schapire</a>  \n",
        "5.  <a href=\"https://explained.ai/gradient-boosting/index.html\"\n",
        "    target=\"_blank\" style=\"text-decoration:none\"><strong>“Understanding\n",
        "    Gradient Boosting Machines”</strong> by Terence Parr and Jeremy\n",
        "    Howard</a>  \n",
        "6.  <a\n",
        "    href=\"https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/\"\n",
        "    target=\"_blank\" style=\"text-decoration:none\"><strong>“A Gentle\n",
        "    Introduction to Gradient Boosting”</strong> by Jason Brownlee</a>\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "**Share on**\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer.php?u=https://mrislambd.github.io/dsandml/posts/gradientboosting/\" target=\"_blank\" style=\"color:#1877F2; text-decoration: none;\">\n",
        "\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://mrislambd.github.io/dsandml/posts/gradientboosting/\" target=\"_blank\" style=\"color:#0077B5; text-decoration: none;\">\n",
        "\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.twitter.com/intent/tweet?url=https://mrislambd.github.io/dsandml/posts/gradientboosting/\" target=\"_blank\" style=\"color:#1DA1F2; text-decoration: none;\">\n",
        "\n",
        "</a>"
      ],
      "id": "28896e8c-f3af-4f97-8d54-60b46fa62a7a"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<script src=\"https://giscus.app/client.js\"\n",
        "        data-repo=\"mrislambd/mrislambd.github.io\" \n",
        "        data-repo-id=\"R_kgDOMV8crA\"\n",
        "        data-category=\"Announcements\"\n",
        "        data-category-id=\"DIC_kwDOMV8crM4CjbQW\"\n",
        "        data-mapping=\"pathname\"\n",
        "        data-strict=\"0\"\n",
        "        data-reactions-enabled=\"1\"\n",
        "        data-emit-metadata=\"0\"\n",
        "        data-input-position=\"bottom\"\n",
        "        data-theme=\"light\"\n",
        "        data-lang=\"en\"\n",
        "        crossorigin=\"anonymous\"\n",
        "        async>\n",
        "</script>"
      ],
      "id": "8918b15e-e85c-4ef3-9313-06f457218b4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [],
      "id": "ca1132a4-140c-428c-80c9-85e7a5a38320"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<script async defer crossorigin=\"anonymous\"\n",
        " src=\"https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v20.0\"></script>"
      ],
      "id": "063d203c-7341-483a-bfe7-dc2ff3cc6bc5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**You may also like**"
      ],
      "id": "42080fd9-25bd-4b56-99d9-06fd5baba8a0"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/opt/hostedtoolcache/Python/3.10.16/x64/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  }
}